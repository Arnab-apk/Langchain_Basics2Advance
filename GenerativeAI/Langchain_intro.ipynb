{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "74b2a7ee",
            "metadata": {},
            "source": [
                "# Getting Started with LangChain v0.1\n",
                "\n",
                "LangChain is a framework for developing applications powered by language models. It enables applications that are:\n",
                "- **Data-aware**: Connect a language model to other sources of data\n",
                "- **Agentic**: Allow a language model to interact with its environment\n",
                "\n",
                "In this notebook, we'll explore the basics of setting up LangChain, integrating different LLM providers (OpenAI, Gemini, Groq), and using key features like Streaming and Batching."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "04e961c2",
            "metadata": {},
            "outputs": [],
            "source": [
                "import langchain\n",
                "print(f\"LangChain Version: {langchain.__version__}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 1. Environment Setup\n",
                "We need to load our API keys from environment variables. A common pattern is to use a `.env` file."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "56a6c2c8",
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "from dotenv import load_dotenv\n",
                "load_dotenv()\n",
                "\n",
                "# Ensure keys are set\n",
                "os.environ[\"OPENAI_API_KEY\"]=os.getenv(\"OPENAI_API_KEY\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "fd8a9d06",
            "metadata": {},
            "source": [
                "### 2. Initializing Chat Models\n",
                "LangChain provides a unified interface for interacting with different Chat Models. `init_chat_model` is a helper to easily initialize models.\n",
                "\n",
                "Below we initialize `gpt-4.1-mini` (likely an OpenRouter model alias)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "6525735d",
            "metadata": {},
            "outputs": [],
            "source": [
                "from langchain.chat_models import init_chat_model\n",
                "model=init_chat_model(\"gpt-4.1-mini\")\n",
                "model"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "7d906eaf",
            "metadata": {},
            "source": [
                "### 3. Using OpenAI Models via OpenRouter\n",
                "Since we are using OpenRouter, we need to explicitly set the `base_url`. This allows us to access various models through the OpenRouter API with an OpenAI-compatible client."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "96b6c1b9",
            "metadata": {},
            "outputs": [],
            "source": [
                "from openai import base_url\n",
                "from langchain.chat_models import init_chat_model\n",
                "import os\n",
                "\n",
                "model = init_chat_model(\n",
                "    model=\"gpt-5\", # Specifying the model name\n",
                "    base_url=\"https://openrouter.ai/api/v1\", # Configuring for OpenRouter\n",
                "    api_key=os.environ.get(\"OPENAI_API_KEY\")\n",
                "    \n",
                ")\n",
                "response=model.invoke(\"write me a simple python code to print hello world\")\n",
                "print(response.content)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "57d1ec59",
            "metadata": {},
            "outputs": [],
            "source": [
                "from langchain_openai import ChatOpenAI\n",
                "import os\n",
                "\n",
                "# Another way to initialize using the ChatOpenAI class directly\n",
                "model = ChatOpenAI(\n",
                "    model=\"openai/gpt-5\", \n",
                "    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
                "    base_url=\"https://openrouter.ai/api/v1\"\n",
                ")\n",
                "\n",
                "response = model.invoke(\"Write me an essay on AI\")\n",
                "print(response.content)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "8b6c9efc",
            "metadata": {},
            "source": [
                "### 4. Integrating Google Gemini\n",
                "LangChain also supports Google's Generative AI models. You need a `GEMINI_API_KEY` for this."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "id": "23c5ff41",
            "metadata": {},
            "outputs": [],
            "source": [
                "os.environ[\"GEMINI_API_KEY\"] = os.getenv(\"GEMINI_API_KEY\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "id": "883a9c45",
            "metadata": {},
            "outputs": [],
            "source": [
                "from langchain.chat_models import init_chat_model\n",
                "\n",
                "# Initialize Gemini model\n",
                "model=init_chat_model(\"google_genai:gemini-2.5-flash-lite\")\n",
                "response=model.invoke(\"Hello, how are you?\")\n",
                "print(response.content)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 29,
            "id": "13353fb7",
            "metadata": {},
            "outputs": [],
            "source": [
                "from langchain_google_genai import ChatGoogleGenerativeAI\n",
                "\n",
                "# Direct initialization using ChatGoogleGenerativeAI\n",
                "model=ChatGoogleGenerativeAI(model=\"gemini-2.5-flash-lite\")\n",
                "response=model.invoke(\"Tell me recent advancements about AI\")\n",
                "print(response.content)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "ef3c7c29",
            "metadata": {},
            "source": [
                "### 5. Integrating Groq\n",
                "Groq provides extremely fast inference. We can use it via `ChatGroq` or `init_chat_model` with `model_provider=\"groq\"`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "e68169ab",
            "metadata": {},
            "outputs": [],
            "source": [
                "from langchain.chat_models import init_chat_model \n",
                "import os  \n",
                "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\n",
                "    "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "bd81d3c4",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize Llama 3 via Groq\n",
                "model = init_chat_model(\"llama-3.1-8b-instant\", model_provider=\"groq\")\n",
                "response = model.invoke(\"Write me a joke on ML algorithms\")\n",
                "print(response.content)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "id": "c76da3ae",
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "from langchain_groq import ChatGroq\n",
                "\n",
                "# Direct initialization using ChatGroq\n",
                "model = ChatGroq(\n",
                "    model=\"qwen/qwen3-32b\"\n",
                ")\n",
                "response = model.invoke(\"Why do parrots speak?\")\n",
                "print(response.content)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "966de885",
            "metadata": {},
            "source": [
                "### 6. Streaming Responses\n",
                "Streaming allows us to receive the output from the model chunk by chunk as it is generated, rather than waiting for the entire response. This improves perceived latency for users."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "1f70aae1",
            "metadata": {},
            "source": [
                "Calling `.stream()` returns an iterator that yields output chunks."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "id": "2107802d",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Example of streaming\n",
                "for chunk in model.stream(\"Write me a 1000 word essay on AI\"):\n",
                "    print(chunk.content, end=\"\", flush=True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "id": "52222722",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Another streaming example\n",
                "for chunk in model.stream(\"Why do parrots have colourful feathers?\"):\n",
                "    print(chunk.content, end=\"\", flush=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "5e75f2c6",
            "metadata": {},
            "source": [
                "### 7. Batching Requests\n",
                "Batching allows you to send multiple independent inputs to the model at once. This can be more efficient than sending them one by one."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "id": "6fca87ba",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Example of batch processing\n",
                "responses = model.batch([\n",
                "    \"What is Quantum Computing?\",\n",
                "    \"How do airplanes fly?\"\n",
                "])\n",
                "\n",
                "for res in responses:\n",
                "    print(f\"\\n--- Response ---\\n{res.content}\")\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}