{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "da4d9e96",
            "metadata": {},
            "source": [
                "### TOOLS\n",
                "Models can request to call tools that perform task such as fetching data from a data base , serching the web etc.\n",
                "1. A scheme includes the name of the tool, the description of the tool, and the parameters of the tool.\n",
                "2. A function is a callable object that implements the tool's behavior.\n",
                "3. A tool is a wrapper around a function that provides additional metadata about the function."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "id": "50957391",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "--- Step 1: Model generates tool call ---\n",
                        "AI Message Content: \n",
                        "Tool Calls: [{'name': 'get_weather', 'args': {'location': 'Kolkata'}, 'id': '0ssc02fye', 'type': 'tool_call'}]\n",
                        "\n",
                        "--- Step 2: Iterate and execute ---\n",
                        "Tool Output: content='It is sunny in Kolkata.' name='get_weather' tool_call_id='0ssc02fye'\n",
                        "\n",
                        "--- Step 3: Final Response ---\n",
                        "The weather in Kolkata is sunny.\n"
                    ]
                }
            ],
            "source": [
                "import os\n",
                "from langchain.chat_models import init_chat_model\n",
                "from langchain_core.tools import tool\n",
                "from langchain_core.messages import HumanMessage, ToolMessage\n",
                "from dotenv import load_dotenv\n",
                "\n",
                "load_dotenv()\n",
                "\n",
                "# Define the tool with hardcoded logic\n",
                "@tool\n",
                "def get_weather(location: str):\n",
                "    \"\"\"Get the weather for a specific location.\"\"\"\n",
                "    location = location.lower()\n",
                "    if \"london\" in location:\n",
                "        return \"It is rainy in London.\"\n",
                "    elif \"new york\" in location:\n",
                "        return \"It is sunny in New York.\"\n",
                "    elif \"kolkata\" in location:\n",
                "        return \"It is sunny in Kolkata.\"\n",
                "    else:\n",
                "        return f\"The weather in {location} is partly cloudy.\"\n",
                "\n",
                "# Initialize model\n",
                "model = init_chat_model(\"llama-3.1-8b-instant\", model_provider=\"groq\")\n",
                "\n",
                "# Bind tool to model\n",
                "model_with_tools = model.bind_tools([get_weather])\n",
                "\n",
                "# --- User's Requested Manual Loop Implementation ---\n",
                "\n",
                "print(\"--- Step 1: Model generates tool call ---\")\n",
                "# step1: Model generates tool call\n",
                "message = [HumanMessage(content=\"What's the weather in Kolkata?\")]\n",
                "ai_msg = model_with_tools.invoke(message)\n",
                "message.append(ai_msg)\n",
                "print(f\"AI Message Content: {ai_msg.content}\")\n",
                "if ai_msg.tool_calls:\n",
                "    print(f\"Tool Calls: {ai_msg.tool_calls}\")\n",
                "\n",
                "# step2: Iterate and execute and collect the results\n",
                "if ai_msg.tool_calls:\n",
                "    print(\"\\n--- Step 2: Iterate and execute ---\")\n",
                "    for tool_call in ai_msg.tool_calls:\n",
                "        # execute the tool with generated arguments\n",
                "        # invoking tool_call directly works if it's a ToolCall dict? No, usually expects args.\n",
                "        # But let's use the explicit args to be safe and clear.\n",
                "        tool_output = get_weather.invoke(tool_call)\n",
                "        print(f\"Tool Output: {tool_output}\")\n",
                "        \n",
                "        # Create a ToolMessage to strictly follow LangChain's message history format\n",
                "        tool_message = ToolMessage(\n",
                "            tool_call_id=tool_call[\"id\"],\n",
                "            content=str(tool_output),\n",
                "            name=tool_call[\"name\"]\n",
                "        )\n",
                "        message.append(tool_message)\n",
                "\n",
                "# step3: Pass the result back to the model for final response\n",
                "print(\"\\n--- Step 3: Final Response ---\")\n",
                "final_response = model_with_tools.invoke(message)\n",
                "# Use .content instead of .text\n",
                "print(final_response.content)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "9e6c160b",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "[HumanMessage(content=\"What's the weather in Kolkata?\", additional_kwargs={}, response_metadata={}),\n",
                            " AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'pc3fwv6z0', 'function': {'arguments': '{\"location\":\"Kolkata\"}', 'name': 'get_weather'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 221, 'total_tokens': 236, 'completion_time': 0.026746335, 'completion_tokens_details': None, 'prompt_time': 0.016632753, 'prompt_tokens_details': None, 'queue_time': 0.045750957, 'total_time': 0.043379088}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_ff2b098aaf', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019c466c-ff41-70e2-b35b-44c6d914e5dc-0', tool_calls=[{'name': 'get_weather', 'args': {'location': 'Kolkata'}, 'id': 'pc3fwv6z0', 'type': 'tool_call'}], invalid_tool_calls=[], usage_metadata={'input_tokens': 221, 'output_tokens': 15, 'total_tokens': 236}),\n",
                            " ToolMessage(content=\"content='It is sunny in Kolkata.' name='get_weather' tool_call_id='pc3fwv6z0'\", name='get_weather', tool_call_id='pc3fwv6z0')]"
                        ]
                    },
                    "execution_count": 2,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "message"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "40b03909",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "RunnableBinding(bound=ChatGroq(profile={'max_input_tokens': 131072, 'max_output_tokens': 8192, 'image_inputs': False, 'audio_inputs': False, 'video_inputs': False, 'image_outputs': False, 'audio_outputs': False, 'video_outputs': False, 'reasoning_output': False, 'tool_calling': True}, client=<groq.resources.chat.completions.Completions object at 0x0000018850FA9FD0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x0000018850FAACF0>, model_name='llama-3.1-8b-instant', model_kwargs={}, groq_api_key=SecretStr('**********')), kwargs={'tools': [{'type': 'function', 'function': {'name': 'get_weather', 'description': 'Get the weather for a specific location.', 'parameters': {'properties': {'location': {'type': 'string'}}, 'required': ['location'], 'type': 'object'}}}]}, config={}, config_factories=[])"
                        ]
                    },
                    "execution_count": 5,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "model_with_tools"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f0e634d0",
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
