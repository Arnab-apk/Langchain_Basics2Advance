{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "da4d9e96",
            "metadata": {},
            "source": [
                "### Introduction to Tools in LangChain\n",
                "\n",
                "Large Language Models (LLMs) are powerful reasoning engines, but they are isolated from the outside world. They can't access real-time data or perform actions on their own. **Tools** bridge this gap.\n",
                "\n",
                "Tools allow models to interact with the world, such as:\n",
                "-   Fetching data from a database or API (e.g., weather, stock prices).\n",
                "-   Searching the web.\n",
                "-   Performing complex calculations.\n",
                "\n",
                "**Core Concepts of a Tool:**\n",
                "1.  **Schema**: A structured definition (JSON) that tells the model the tool's **name**, **description**, and **arguments** (inputs). The model uses this to \"decide\" if and how to call the tool.\n",
                "2.  **Function**: The actual Python code that performs the logic (the \"work\").\n",
                "3.  **Tool Object**: A wrapper in LangChain that combines the function and schema."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 1. Defining a Tool with `@tool`\n",
                "\n",
                "The `@tool` decorator is the easiest way to define a custom tool. It automatically extracts the schema from your Python function:\n",
                "-   **Name**: Taken from the function name (e.g., `get_weather`).\n",
                "-   **Description**: Taken from the function's docstring. **Crucial**: This tells the model *when* to use the tool.\n",
                "-   **Arguments**: Taken from the function arguments and type hints (e.g., `location: str`)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "id": "50957391",
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "from langchain.chat_models import init_chat_model\n",
                "from langchain_core.tools import tool\n",
                "from langchain_core.messages import HumanMessage, ToolMessage\n",
                "from dotenv import load_dotenv\n",
                "\n",
                "load_dotenv()\n",
                "\n",
                "# Define the tool with hardcoded logic\n",
                "@tool\n",
                "def get_weather(location: str):\n",
                "    \"\"\"Get the weather for a specific location.\"\"\"\n",
                "    location = location.lower()\n",
                "    if \"london\" in location:\n",
                "        return \"It is rainy in London.\"\n",
                "    elif \"new york\" in location:\n",
                "        return \"It is sunny in New York.\"\n",
                "    elif \"kolkata\" in location:\n",
                "        return \"It is sunny in Kolkata.\"\n",
                "    else:\n",
                "        return f\"The weather in {location} is partly cloudy.\""
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2. Binding Tools to the Model\n",
                "\n",
                "Start by initializing a chat model. Then, use `.bind_tools([tool_list])`.\n",
                "\n",
                "**Why bind tools?**\n",
                "Standard LLMs generate text. Models fine-tuned for tool calling (like `llama-3.1-8b`, `gpt-4`, etc.) need to know what tools are available. `bind_tools` converts your Python tool definitions into the specific JSON format the model provider expects (e.g., OpenAI function calling schema)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "40b03909",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize model\n",
                "model = init_chat_model(\"llama-3.1-8b-instant\", model_provider=\"groq\")\n",
                "\n",
                "# Bind tool to model\n",
                "model_with_tools = model.bind_tools([get_weather])\n",
                "\n",
                "model_with_tools"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3. The Agent Loop (Manual Implementation)\n",
                "\n",
                "Modern Agents (like those in **LangGraph**) typically run a loop. Here, we implement a single iteration manually to understand what happens under the hood.\n",
                "\n",
                "**The Workflow:**\n",
                "\n",
                "1.  **Thinking (Model Generation)**: We send the user's query to the model. The model analyzes the query and the available tools.\n",
                "2.  **Tool Call Generation**: Instead of a text response, the model returns a **Tool Call**. This contains:\n",
                "    -   The name of the tool to call (`get_weather`).\n",
                "    -   The arguments to pass (`{\"location\": \"Kolkata\"}`).\n",
                "    -   A unique ID for the call.\n",
                "3.  **Execution**: The application (us) detects the tool call. We invoke the actual Python function `get_weather` using the provided arguments.\n",
                "4.  **Observation**: We get the output string (\"It is sunny in Kolkata.\"). We must pass this back to the model as a `ToolMessage` linked to the original call ID.\n",
                "5.  **Final Response**: The model receives the tool's output. It uses this information to generate a natural language response for the user."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "9e6c160b",
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"--- Step 1: Model generates tool call ---\")\n",
                "# step1: Model generates tool call\n",
                "message = [HumanMessage(content=\"What's the weather in Kolkata?\")]\n",
                "ai_msg = model_with_tools.invoke(message)\n",
                "message.append(ai_msg)\n",
                "print(f\"AI Message Content: {ai_msg.content}\")\n",
                "if ai_msg.tool_calls:\n",
                "    print(f\"Tool Calls: {ai_msg.tool_calls}\")\n",
                "\n",
                "# step2: Iterate and execute and collect the results\n",
                "if ai_msg.tool_calls:\n",
                "    print(\"\\n--- Step 2: Iterate and execute ---\")\n",
                "    for tool_call in ai_msg.tool_calls:\n",
                "        # execute the tool with generated arguments\n",
                "        # invoking tool_call directly works if it's a ToolCall dict? No, usually expects args.\n",
                "        # But let's use the explicit args to be safe and clear.\n",
                "        tool_output = get_weather.invoke(tool_call)\n",
                "        print(f\"Tool Output: {tool_output}\")\n",
                "        \n",
                "        # Create a ToolMessage to strictly follow LangChain's message history format\n",
                "        tool_message = ToolMessage(\n",
                "            tool_call_id=tool_call[\"id\"],\n",
                "            content=str(tool_output),\n",
                "            name=tool_call[\"name\"]\n",
                "        )\n",
                "        message.append(tool_message)\n",
                "\n",
                "# step3: Pass the result back to the model for final response\n",
                "print(\"\\n--- Step 3: Final Response ---\")\n",
                "final_response = model_with_tools.invoke(message)\n",
                "# Use .content instead of .text\n",
                "print(final_response.content)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f0e634d0",
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}