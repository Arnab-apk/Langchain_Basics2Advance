{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "da4d9e96",
            "metadata": {},
            "source": [
                "### Introduction to Tools in LangChain\n",
                "\n",
                "Large Language Models (LLMs) are powerful, but they can't access real-time data or perform actions on their own. **Tools** bridge this gap.\n",
                "\n",
                "Models can request to call tools that perform tasks such as fetching data from a database, searching the web, or calculating values.\n",
                "\n",
                "Key concepts:\n",
                "1.  **Schema**: Defines the tool's name, description, and parameters so the model knows how to use it.\n",
                "2.  **Function**: The actual Python code that performs the task.\n",
                "3.  **Tool**: A wrapper around the function that provides the schema and metadata."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 1. Defining a Tool"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "id": "50957391",
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "from langchain.chat_models import init_chat_model\n",
                "from langchain_core.tools import tool\n",
                "from langchain_core.messages import HumanMessage, ToolMessage\n",
                "from dotenv import load_dotenv\n",
                "\n",
                "load_dotenv()\n",
                "\n",
                "# Define the tool with hardcoded logic\n",
                "@tool\n",
                "def get_weather(location: str):\n",
                "    \"\"\"Get the weather for a specific location.\"\"\"\n",
                "    location = location.lower()\n",
                "    if \"london\" in location:\n",
                "        return \"It is rainy in London.\"\n",
                "    elif \"new york\" in location:\n",
                "        return \"It is sunny in New York.\"\n",
                "    elif \"kolkata\" in location:\n",
                "        return \"It is sunny in Kolkata.\"\n",
                "    else:\n",
                "        return f\"The weather in {location} is partly cloudy.\""
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2. Binding Tools to the Model\n",
                "We need to tell the model about the tools available to it. We do this using `.bind_tools()`. This converts the tool definitions into a format the model understands (like OpenAI function calling schema)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "40b03909",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize model\n",
                "model = init_chat_model(\"llama-3.1-8b-instant\", model_provider=\"groq\")\n",
                "\n",
                "# Bind tool to model\n",
                "model_with_tools = model.bind_tools([get_weather])\n",
                "\n",
                "model_with_tools"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3. The Agent Loop (Manual Implementation)\n",
                "When an agent runs, it typically follows this loop:\n",
                "1.  **Thinking**: The model receives the user query and decides if it needs to call a tool.\n",
                "2.  **Tool Call Generation**: If a tool is needed, the model returns a \"tool call\" request (name of tool + arguments).\n",
                "3.  **Execution**: We (or the agent runtime) execute the tool with the provided arguments.\n",
                "4.  **Observation**: The output of the tool is fed back to the model.\n",
                "5.  **Final Response**: The model uses the tool output to generate a final natural language response for the user.\n",
                "\n",
                "Below is a manual implementation of this loop:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "9e6c160b",
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"--- Step 1: Model generates tool call ---\")\n",
                "# step1: Model generates tool call\n",
                "message = [HumanMessage(content=\"What's the weather in Kolkata?\")]\n",
                "ai_msg = model_with_tools.invoke(message)\n",
                "message.append(ai_msg)\n",
                "print(f\"AI Message Content: {ai_msg.content}\")\n",
                "if ai_msg.tool_calls:\n",
                "    print(f\"Tool Calls: {ai_msg.tool_calls}\")\n",
                "\n",
                "# step2: Iterate and execute and collect the results\n",
                "if ai_msg.tool_calls:\n",
                "    print(\"\\n--- Step 2: Iterate and execute ---\")\n",
                "    for tool_call in ai_msg.tool_calls:\n",
                "        # execute the tool with generated arguments\n",
                "        # invoking tool_call directly works if it's a ToolCall dict? No, usually expects args.\n",
                "        # But let's use the explicit args to be safe and clear.\n",
                "        tool_output = get_weather.invoke(tool_call)\n",
                "        print(f\"Tool Output: {tool_output}\")\n",
                "        \n",
                "        # Create a ToolMessage to strictly follow LangChain's message history format\n",
                "        tool_message = ToolMessage(\n",
                "            tool_call_id=tool_call[\"id\"],\n",
                "            content=str(tool_output),\n",
                "            name=tool_call[\"name\"]\n",
                "        )\n",
                "        message.append(tool_message)\n",
                "\n",
                "# step3: Pass the result back to the model for final response\n",
                "print(\"\\n--- Step 3: Final Response ---\")\n",
                "final_response = model_with_tools.invoke(message)\n",
                "# Use .content instead of .text\n",
                "print(final_response.content)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f0e634d0",
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}